{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/devanshk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import nltk \n",
    "\n",
    "# Ensure NLTK tokenization is available\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Load spaCy English tokenizer\n",
    "# !python -m spacy download en_core_web_sm #run the first time\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"tagger\"])\n",
    "# Define input folder containing legal contracts\n",
    "INPUT_FILE = \"concatenated_text.txt\"\n",
    "# INPUT_FILE = \"sample.txt\"\n",
    "OUTPUT_FILE = \"output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_TYPE = \"spacy\"\n",
    "CHUNK_SIZE = 500_000  # Process 500,000 characters at a time for large text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_in_chunks(file_path, chunk_size=CHUNK_SIZE):\n",
    "    \"\"\"Yields chunks of text from a large file to avoid memory overload.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                yield chunk\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenizer(text):\n",
    "    \"\"\"\n",
    "    Tokenizes text using a custom regex-based approach.\n",
    "    - Splits words while keeping punctuation as separate tokens.\n",
    "    - Handles hyphenated words correctly.\n",
    "    \"\"\"\n",
    "    token_pattern = r\"\\w+(?:-\\w+)*|[.,!?;()\\[\\]{}:\\\"\\'“”‘’]\"\n",
    "    return re.findall(token_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_tokenizer(text):\n",
    "    \"\"\"Tokenizes text using a custom regex-based approach.\"\"\"\n",
    "    token_pattern = r\"\\w+(?:-\\w+)*|[.,!?;()\\[\\]{}:\\\"\\'“”‘’]\"\n",
    "    return re.findall(token_pattern, text)\n",
    "\n",
    "def nltk_tokenizer(text):\n",
    "    \"\"\"Tokenizes text using NLTK's word_tokenize function.\"\"\"\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def spacy_tokenizer(text_chunks):\n",
    "    \"\"\"Processes text chunks using spaCy's tokenizer.\"\"\"\n",
    "    tokens = []\n",
    "    for chunk in text_chunks:\n",
    "        doc = nlp(chunk)\n",
    "        tokens.extend(token.text for token in doc)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text_chunks, tokenizer_type=\"spacy\"):\n",
    "    \"\"\"\n",
    "    Tokenizes text using the selected tokenizer.\n",
    "    - For regex & NLTK: Joins chunks and tokenizes once.\n",
    "    - For spaCy: Processes chunks separately for efficiency.\n",
    "    \"\"\"\n",
    "    if tokenizer_type == \"regex\":\n",
    "        return regex_tokenizer(\" \".join(text_chunks))\n",
    "    elif tokenizer_type == \"nltk\":\n",
    "        return nltk_tokenizer(\" \".join(text_chunks))\n",
    "    elif tokenizer_type == \"spacy\":\n",
    "        return spacy_tokenizer(text_chunks)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid tokenizer type. Use 'regex', 'nltk', or 'spacy'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_tokens_to_file(tokens, output_file):\n",
    "    \"\"\"Writes tokens to an output file, one token per line.\"\"\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for token in tokens:\n",
    "            f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ext of length 26807133 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the nlp.max_length limit. The limit is in number of characters, so you can check whether your inputs are too long by checking len(text).\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"ext of length 26807133 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the nlp.max_length limit. The limit is in number of characters, so you can check whether your inputs are too long by checking len(text).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_file():\n",
    "    \"\"\"Reads text in chunks, tokenizes, and writes output.\"\"\"\n",
    "    text_chunks = list(read_file_in_chunks(INPUT_FILE))\n",
    "    if not text_chunks:\n",
    "        return\n",
    "\n",
    "    print(f\"Processing {len(text_chunks)} chunks...\")\n",
    "    \n",
    "    tokens = tokenize_text(text_chunks, TOKENIZER_TYPE)\n",
    "    write_tokens_to_file(tokens, OUTPUT_FILE)\n",
    "\n",
    "    print(f\"Tokenization complete using '{TOKENIZER_TYPE}'. Output written to {OUTPUT_FILE}\")\n",
    "    print(f\"Total tokens: {len(tokens):,d}\")\n",
    "    print(f\"First 20 tokens: {tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 54 chunks...\n",
      "Tokenization complete using 'spacy'. Output written to output.txt\n",
      "Total tokens: 4,997,689\n",
      "First 20 tokens: ['CO', '-', 'BRANDING', 'AND', 'ADVERTISING', 'AGREEMENT', '\\n\\n', 'THIS', 'CO', '-', 'BRANDING', 'AND', 'ADVERTISING', 'AGREEMENT', '(', 'the', '\"', 'Agreement', '\"', ')']\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    process_text_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
